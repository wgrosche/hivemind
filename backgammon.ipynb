{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions as distributions\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Implementing the backgammon game\n",
    "\n",
    "White pieces are represented by 1 and black pieces by -1\n",
    "\n",
    "White starts the game\n",
    "\n",
    "'''\n",
    "\n",
    "class Board():\n",
    "    def __init__(self):\n",
    "        self.setup()\n",
    "        self.turn = 0\n",
    "        self.startpool = [0,0]\n",
    "\n",
    "    def print_board(self):\n",
    "        print(self.board)\n",
    "\n",
    "    def setup(self):\n",
    "        self.board = [2,0,0,0,0,-5,0,-3,0,0,0,5,-5,0,0,0,3,0,5,0,0,0,0,-2]\n",
    "\n",
    "    def bear_off(self):\n",
    "        if self.turn % 2 == 0:\n",
    "            if self.startpool[0] == 0:\n",
    "                bull_count = 0\n",
    "                for i in self.board[:-6]:\n",
    "                    bull_count += max([0,i])\n",
    "                if bull_count == 0:\n",
    "                    return True\n",
    "        elif self.turn % 2 == 1:\n",
    "            if self.startpool[1] == 0:\n",
    "                bull_count = 0\n",
    "                for i in self.board[6:]:\n",
    "                    bull_count += min([0,i])\n",
    "                if bull_count == 0:\n",
    "                    return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "\n",
    "    def move_(self, start, roll):\n",
    "        if start+roll > len(self.board):\n",
    "            if not self.bear_off():\n",
    "                return False\n",
    "        if start > (-1)**self.turn*(start+roll):\n",
    "            print(\"You can only move pieces forward\")\n",
    "            return False\n",
    "        elif np.sign(self.board[start]) != (-1)**self.turn:\n",
    "            print(\"You can only move pieces of your own colour\")\n",
    "            return False\n",
    "        elif np.sign(self.board[(start+roll)]) == np.sign(self.board[start]):\n",
    "            self.board[(start+roll)] += np.sign(self.board[start])\n",
    "            self.board[start] -= np.sign(self.board[start])\n",
    "            return True\n",
    "\n",
    "        elif np.abs(self.board[(start+roll)]) > 1:\n",
    "            print(\"You can't move to a space occupied by two or more opposing pieces\")\n",
    "            return False\n",
    "\n",
    "        else:\n",
    "            self.startpool[self.turn % 2 + 1] += self.board[(start+roll)]\n",
    "            self.board[(start+roll)] = 0\n",
    "            self.board[(start+roll)] += np.sign(self.board[start])\n",
    "            self.board[start] -= np.sign(self.board[start])\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "    def roll(self):\n",
    "        roll_1 = np.random.randint(1, 7)\n",
    "        roll_2 = np.random.randint(1, 7)\n",
    "\n",
    "        if roll_1 == roll_2:\n",
    "            return [roll_1, roll_2, roll_1, roll_2]\n",
    "        else:\n",
    "            return [roll_1, roll_2, 0, 0]\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.board + self.startpool  + self.roll()\n",
    "\n",
    "    def take_turn(self, agent):\n",
    "        state = self.get_state()\n",
    "        for move in  agent.take_turn(state):\n",
    "            self.move_(move[0], move[1])\n",
    "            self.check_victory()\n",
    "        return True\n",
    "\n",
    "\n",
    "    def check_victory(self):\n",
    "        if self.turn % 2 == 0:\n",
    "            if self.startpool[0] == 0:\n",
    "                bull_count = 0\n",
    "                for i in self.board:\n",
    "                    bull_count += max([0, i])\n",
    "                if bull_count == 0:\n",
    "                    return True\n",
    "        elif self.turn % 2 == 1:\n",
    "            if self.startpool[1] == 0:\n",
    "                bull_count = 0\n",
    "                for i in self.board:\n",
    "                    bull_count += min([0,i])\n",
    "                if bull_count == 0:\n",
    "                    return True\n",
    "        else:\n",
    "            return False  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, actor, critic):\n",
    "        super().__init__()\n",
    "\n",
    "        self.actor = actor\n",
    "        self.critic = critic\n",
    "\n",
    "    def forward(self, state):\n",
    "\n",
    "        action_pred = self.actor(state)\n",
    "        value_pred = self.critic(state)\n",
    "\n",
    "        return action_pred, value_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = Board()\n",
    "\n",
    "input_dim = np.shape(board.get_state())\n",
    "hidden_dim = 32\n",
    "output_dim = 8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        torch.nn.init.kaiming_normal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(env, policy, optimizer, discount_factor, device):\n",
    "\n",
    "    policy.train()\n",
    "\n",
    "    log_prob_actions = []\n",
    "    entropies = []\n",
    "    value_preds = []\n",
    "    rewards = []\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "        action_pred, value_pred = policy(state)\n",
    "\n",
    "        action_prob = F.softmax(action_pred, dim=-1)\n",
    "\n",
    "        dist = distributions.Categorical(action_prob)\n",
    "\n",
    "        action = dist.sample()\n",
    "        log_prob_action = dist.log_prob(action)\n",
    "\n",
    "        entropy = dist.entropy()\n",
    "\n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        log_prob_actions.append(log_prob_action)\n",
    "        entropies.append(entropy)\n",
    "        value_preds.append(value_pred.squeeze(0))\n",
    "        rewards.append(reward)\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "    log_prob_actions = torch.cat(log_prob_actions)\n",
    "    entropies = torch.cat(entropies)\n",
    "    value_preds = torch.cat(value_preds)\n",
    "\n",
    "    returns = calculate_returns(rewards, discount_factor, device)\n",
    "    advantages = calculate_advantages(returns, value_preds)\n",
    "\n",
    "    loss = update_policy(advantages, log_prob_actions,\n",
    "                         returns, value_preds, entropies, optimizer)\n",
    "\n",
    "    return loss, episode_reward\n",
    "\n",
    "\n",
    "def calculate_returns(rewards, discount_factor, device, normalize=True):\n",
    "\n",
    "    returns = []\n",
    "    R = 0\n",
    "\n",
    "    for r in reversed(rewards):\n",
    "        R = r + R * discount_factor\n",
    "        returns.insert(0, R)\n",
    "\n",
    "    returns = torch.tensor(returns).to(device)\n",
    "\n",
    "    if normalize:\n",
    "        returns = (returns - returns.mean()) / returns.std()\n",
    "\n",
    "    return returns\n",
    "\n",
    "\n",
    "def calculate_advantages(returns, pred_values, normalize=True):\n",
    "\n",
    "    advantages = returns - pred_values\n",
    "\n",
    "    if normalize:\n",
    "\n",
    "        advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "    return advantages\n",
    "\n",
    "\n",
    "def update_policy(advantages, log_prob_actions, returns, value_preds, entropies, optimizer):\n",
    "\n",
    "    returns = returns.detach()\n",
    "\n",
    "    policy_loss = -(advantages * log_prob_actions).mean()\n",
    "    value_loss = F.smooth_l1_loss(returns, value_preds)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss = policy_loss + value_loss * 0.5 - entropies.mean() * 0.01\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def evaluate(env, policy, device):\n",
    "\n",
    "    policy.eval()\n",
    "\n",
    "    done = False\n",
    "    episode_reward = 0\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    while not done:\n",
    "\n",
    "        state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            action_pred, _ = policy(state)\n",
    "\n",
    "            action_prob = F.softmax(action_pred, dim=-1)\n",
    "\n",
    "        action = torch.argmax(action_prob, dim=-1)\n",
    "\n",
    "        state, reward, done, _ = env.step(action.item())\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "    return episode_reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb Cell 7'\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000008?line=6'>7</a>\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000008?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m run \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_runs):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000008?line=10'>11</a>\u001b[0m     actor \u001b[39m=\u001b[39m MLP(input_dim, hidden_dim, output_dim)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000008?line=11'>12</a>\u001b[0m     critic \u001b[39m=\u001b[39m MLP(input_dim, hidden_dim, \u001b[39m1\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000008?line=12'>13</a>\u001b[0m     actor_critic \u001b[39m=\u001b[39m ActorCritic(actor, critic)\n",
      "\u001b[1;32m/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb Cell 3'\u001b[0m in \u001b[0;36mMLP.__init__\u001b[0;34m(self, input_dim, hidden_dim, output_dim)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000007?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, input_dim, hidden_dim, output_dim):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000007?line=3'>4</a>\u001b[0m     \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000007?line=5'>6</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(input_dim, hidden_dim)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/friedrichwilkegrosche/hivemind/backgammon.ipynb#ch0000007?line=6'>7</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_dim, output_dim)\n",
      "File \u001b[0;32m~/miniforge3/envs/hivemind/lib/python3.10/site-packages/torch/nn/modules/linear.py:85\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     84\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 85\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     86\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "n_runs = 5\n",
    "max_episodes = 300\n",
    "discount_factor = 0.99\n",
    "\n",
    "train_rewards = torch.zeros(n_runs, max_episodes)\n",
    "test_rewards = torch.zeros(n_runs, max_episodes)\n",
    "device = torch.device('cpu')\n",
    "\n",
    "for run in range(n_runs):\n",
    "\n",
    "    actor = MLP(input_dim, hidden_dim, output_dim)\n",
    "    critic = MLP(input_dim, hidden_dim, 1)\n",
    "    actor_critic = ActorCritic(actor, critic)\n",
    "    actor_critic = actor_critic.to(device)\n",
    "    actor_critic.apply(init_weights)\n",
    "    optimizer = optim.Adam(actor_critic.parameters(), lr=1e-2)\n",
    "\n",
    "    for episode in tqdm.tqdm(range(max_episodes), desc=f'Run: {run}'):\n",
    "\n",
    "        loss, train_reward = train(\n",
    "            train_env, actor_critic, optimizer, discount_factor, device)\n",
    "\n",
    "        test_reward = evaluate(test_env, actor_critic, device)\n",
    "\n",
    "        train_rewards[run][episode] = train_reward\n",
    "        test_rewards[run][episode] = test_reward\n",
    "\n",
    "\n",
    "idxs = range(max_episodes)\n",
    "fig, ax = plt.subplots(1, figsize=(10, 6))\n",
    "ax.plot(idxs, test_rewards.mean(0))\n",
    "ax.fill_between(idxs, test_rewards.min(0).values,\n",
    "                test_rewards.max(0).values, alpha=0.1)\n",
    "ax.set_xlabel('Steps')\n",
    "ax.set_ylabel('Rewards')\n",
    "\n",
    "x = torch.randn(2, 10)\n",
    "y = torch.randn(2, 10)\n",
    "print(F.smooth_l1_loss(x, y))\n",
    "print(F.mse_loss(x, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('hivemind')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68c606508e586a01c3c8026c75f96ff0177306a24be15fc02ae4ce9a5ad27c71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
